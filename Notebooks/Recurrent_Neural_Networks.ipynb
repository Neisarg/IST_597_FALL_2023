{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>IST 597 Foundations of Deep Learning</center></h1>\n",
        "\n",
        "---\n",
        "\n",
        "<h2><center>Recurrent Neural Networks</center><h2>\n",
        "<h3><center>Neisarg Dave</center><h3>"
      ],
      "metadata": {
        "id": "0wigSOhXT6f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init, Parameter\n",
        "import math"
      ],
      "metadata": {
        "id": "UlSGQlqZge9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Long Short Term Memory (LSTM)\n",
        "+ RNN cell composed of gates\n",
        "+ 3 gates\n",
        "  + Input Gate\n",
        "  + Output Gate\n",
        "  + Forget Gate\n",
        "+ Hidden State is a tuple: $(h, c)$\n",
        "+ $h$ acts as the representative of RNN state at given step\n",
        "+ $c$ acts as a scratchpad to erase and write something at each step\n",
        "\n",
        "+ Gate equations:\n",
        "$$\n",
        "  i_t = \\sigma(U_ix_t + W_ih_{t-1} + b_i) \\\\\n",
        "  f_t = \\sigma(U_fx_t + W_fh_{t-1} + b_f) \\\\\n",
        "  o_t = \\sigma(U_ox_t + W_oh_{t-1} + b_o)\n",
        "$$\n",
        "\n",
        "+ New information to be updated on $c$\n",
        "\n",
        "$$\n",
        "  \\tilde{c}_t = \\tanh(U_cx_t + W_ch_{t-1} + b_c)\n",
        "$$\n",
        "\n",
        "+ Update information to scratchpad\n",
        "\n",
        "$$\n",
        "  c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
        "$$\n",
        "\n",
        "+ Update state vector\n",
        "\n",
        "$$\n",
        "  h_t = o_t \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "+ **Peephole Architecture**\n",
        "  + Let the gate equations look at the cell scratchpad $c$.\n",
        "  + $c$ here now acts as cell state along with $h$\n",
        "  $$\n",
        "  i_t = \\sigma(U_ix_t + W_ih_{t-1} + V_ic_{t-1} + b_i) \\\\\n",
        "  f_t = \\sigma(U_fx_t + W_fh_{t-1} + V_fc_{t-1} + b_f) \\\\\n",
        "  o_t = \\sigma(U_ox_t + W_oh_{t-1} + V_oc_{t} + b_o)\n",
        "  $$\n",
        "\n",
        "+ Torch API\n",
        "  + https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  + https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html"
      ],
      "metadata": {
        "id": "CmYveKULUAJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.U = Parameter(torch.empty(self.in_features, 4*self.out_features))\n",
        "        self.W = Parameter(torch.empty(self.out_features, 4*self.out_features))\n",
        "        self.bias = Parameter(torch.empty(4*self.out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.normal_(self.U, mean=0, std=0.1)\n",
        "        init.normal_(self.W, mean=0, std=0.1)\n",
        "\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.U.t())\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden, reverse = False):\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            hidden = self.inner(input[i], hidden)\n",
        "            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n",
        "\n",
        "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
        "\n",
        "        return hidden, output\n",
        "\n",
        "    def inner(self,  input, hidden):\n",
        "        h,c = hidden\n",
        "        all_sum = torch.matmul(input, self.U) + torch.matmul(h, self.W) + self.bias\n",
        "        i, f, c_hat, o = torch.chunk(all_sum, 4, dim=1)\n",
        "        i, f, c_hat, o = torch.sigmoid(i), torch.sigmoid(f), torch.tanh(c_hat), torch.sigmoid(o)\n",
        "        c = f*c + i*c_hat\n",
        "        h = o*torch.tanh(c)\n",
        "        hidden = (h, c)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "mcwF6GAQiDMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gated Recurrent Unit (GRU)\n",
        "+ Simplified Gated Architecture\n",
        "+ Hidden State is represented by only one vector\n",
        "+ Two gates : $z$ and $r$\n",
        "$$\n",
        "  z_t = \\sigma(U_zx_t + W_zh_{t-1} + b_z) \\\\\n",
        "  r_t = \\sigma(U_rx_t + W_rh_{t-1} + b_r) \\\\\n",
        "$$\n",
        "\n",
        "+ State Update\n",
        "$$\n",
        "  \\tilde{h}_t = tanh(U_hx_t + W_h(r_t \\odot h_{t-1}) + b_h) \\\\\n",
        "  h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
        "$$"
      ],
      "metadata": {
        "id": "P4ngX_6zUDaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GRU, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.U = Parameter(torch.empty(self.in_features, 2*self.out_features))\n",
        "        self.W = Parameter(torch.empty(self.out_features, 2*self.out_features))\n",
        "        self.Uh = Parameter(torch.empty(self.in_features, self.out_features))\n",
        "        self.Wh = Parameter(torch.empty(self.out_features, self.out_features))\n",
        "        self.bias = Parameter(torch.empty(2*self.out_features))\n",
        "        self.bh = Parameter(torch.empty(self.out_features))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.normal_(self.U, mean=0, std=0.1)\n",
        "        init.normal_(self.W, mean=0, std=0.1)\n",
        "        init.normal_(self.Uh, mean=0, std=0.1)\n",
        "        init.normal_(self.Wh, mean=0, std=0.1)\n",
        "\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.U.t())\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias, -bound, bound)\n",
        "        init.uniform_(self.bh, -bound, bound)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden, reverse = False):\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            hidden = self.inner(input[i], hidden)\n",
        "            output.append(hidden)\n",
        "\n",
        "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
        "\n",
        "        return hidden, output\n",
        "\n",
        "    def inner(self,  input, hidden):\n",
        "        all_sum = torch.matmul(input, self.U) + torch.matmul(hidden, self.W) + self.bias\n",
        "        z, r = torch.chunk(all_sum, 2, dim=1)\n",
        "        z, r = torch.sigmoid(z), torch.sigmoid(r)\n",
        "        h_hat = torch.tanh( torch.matmul(input, self.Uh) + torch.matmul(r*hidden, self.Wh)+ self.bh)\n",
        "        hidden = (1-z)*hidden + z*h_hat\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "MaHsLYTUjRNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2nd Order RNN\n",
        "\n",
        "+ weight matrix is a 3D tensor\n",
        "$$\n",
        "  h_{t, i} = \\sigma(\\sum_{j, k} W_{ijk}x_{t, j}h_{t-1, k} + b_i)\n",
        "$$"
      ],
      "metadata": {
        "id": "sYXLvoA3UF_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class O2RNN(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(O2RNN, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        weight = torch.empty(in_features, out_features, out_features)\n",
        "\n",
        "        self.weight = Parameter(weight.view(in_features, -1))\n",
        "        self.bias = Parameter(torch.empty(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.normal_(self.weight, mean=0, std=0.1)\n",
        "        init.constant_(self.bias, 0.01)\n",
        "\n",
        "    def inner(self, input, hidden=None):\n",
        "        WX = F.linear(input, self.weight.transpose(1, 0))\n",
        "        WX = WX.view(-1, self.out_features, self.out_features)\n",
        "        WHX = WX.bmm(hidden.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        return torch.sigmoid(WHX + self.bias)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            hidden = self.inner(input[i], hidden)\n",
        "            output.append(hidden)\n",
        "\n",
        "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
        "        return hidden, output"
      ],
      "metadata": {
        "id": "iZFuP2ubpBlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiplicative Integration RNNs\n",
        "+ Integration of two information flows by hadamard product\n",
        "+ RNN state update equation:\n",
        "$$\n",
        "  h_t = \\phi(Ux_t + Wh_{t-1} + b)\n",
        "$$\n",
        "\n",
        "+ MI-RNN:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  h_t = \\phi( \\ & \\alpha \\odot Ux \\odot Wh_{t-1} \\\\\n",
        "                  &+ \\beta_1 \\odot Ux \\\\\n",
        "                  &+ \\beta_2 \\odot Wh_{t-1} \\\\\n",
        "                  &+ b \\ )\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "+ Similarly we can now define MI-LSTM and MI-GRU\n",
        "+ https://arxiv.org/pdf/1606.06630.pdf\n"
      ],
      "metadata": {
        "id": "27YpamLsUVQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MIRNN(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(MIRNN, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.U = Parameter(torch.empty(self.in_features, self.out_features))\n",
        "        self.W = Parameter(torch.empty(self.out_features, self.out_features))\n",
        "\n",
        "        self.alpha = Parameter(torch.empty(out_features))\n",
        "        self.beta1 = Parameter(torch.empty(out_features))\n",
        "        self.beta2 = Parameter(torch.empty(out_features))\n",
        "\n",
        "        self.bias = Parameter(torch.empty(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.normal_(self.U, mean=0, std=0.1)\n",
        "        init.normal_(self.W, mean=0, std=0.1)\n",
        "\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.U.t())\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias, -bound, bound)\n",
        "        init.uniform_(self.alpha, -bound, bound)\n",
        "        init.uniform_(self.beta1, -bound, bound)\n",
        "        init.uniform_(self.beta2, -bound, bound)\n",
        "\n",
        "    def inner(self, input, hidden):\n",
        "        ux = torch.matmul(input, self.U)\n",
        "        wh = torch.matmul(hidden, self.W)\n",
        "        hidden = torch.sigmoid(self.alpha * ux * wh + self.beta1*ux + self.beta2*wh + self.bias)\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            hidden = self.inner(input[i], hidden)\n",
        "            output.append(hidden)\n",
        "\n",
        "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
        "        return hidden, output"
      ],
      "metadata": {
        "id": "IPffZ78wlG1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global Attention\n",
        "\n",
        "Attention mechanism is composed of two steps:\n",
        "1. Calculating similarity scores for concerned tensors\n",
        "  + Target States ($h^{(t)}$) : Decoder hidden states\n",
        "  + Source States ($h^{(s)}$) : Encoder hidden states\n",
        "  + alignment scores $a = softmax(f(h^{(t)}, h^{(s)}))$\n",
        "\n",
        "2. Constructing a *Context Vector* that has the information about this attention\n",
        "  + Context Vector $ c = \\sum_i a_ih^{(s)}_{i}$\n",
        "\n",
        "**Dot Attention**\n",
        "$$f(h^{(t)}_i, h^{(s)}_i) = h^{(t)T}_i h^{(s)}_i$$\n",
        "\n",
        "**Concat Attention**\n",
        "$$f(h^{(t)}_i, h^{(s)}_i) = v^TW[h^{(t)}_i ; h^{(s)}_i]$$\n",
        "\n",
        "**General Attention**\n",
        "$$f(h^{(t)}_i, h^{(s)}_i) = h^{(t)}_iWh^{(s)}_i$$\n",
        "\n",
        "### Local Attention\n",
        "\n",
        "$$\n",
        "a_t(s) = f(h^{(t)}, h^{(s)})\\ exp(-\\frac{(s - p_t)^2}{2\\sigma^2})\n",
        "$$\n",
        "**Further Reading**:\n",
        "+ https://arxiv.org/pdf/1508.04025.pdf"
      ],
      "metadata": {
        "id": "qjGbPvqgokmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bahdanau Attention\n",
        "\n",
        "$$ h_t = RNN([x_t; c_{t-1}], h_{t-1})$$"
      ],
      "metadata": {
        "id": "6kG-m0fR1Z-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Luong Manning Attention"
      ],
      "metadata": {
        "id": "ITBkDmDuoezD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QHmXRfPWOww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\tilde{h}_t = \\tanh(W[c_t ;h_t]) $$\n",
        "$$P(y_t | y_{<t}, x) = softmax(W_h \\tilde{h}_t) $$"
      ],
      "metadata": {
        "id": "XI2tYNjz0btH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongManingDotAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LuongManingDotAttention, self).__init__()\n",
        "\n",
        "    def forward(self, encoder_hidden, enc_mask, decoder_hidden):\n",
        "        \"\"\"\n",
        "        encoder_hidden : n_samples x encoder_seq_len x state_dim\n",
        "        enc_mask : n_samples x encoder_seq_len , 1 for valid sequence and 0 for padding\n",
        "        decoder_hidden : n_samples x decoder_sequence_len x state_dim\n",
        "        \"\"\"\n",
        "        a = torch.bmm(encoder_hidden, decoder_hidden.transpose(2, 1))\n",
        "        enc_mask = ~enc_mask.unsqueeze(dim = -1).expand(*c.shape)\n",
        "        enc_mask = enc_mask.masked_fill_(enc_mask, float(\"-inf\"))\n",
        "        a = a + enc_mask\n",
        "        a = torch.softmax(a, dim = 1)\n",
        "        c = torch.bmm(a.transpose(2,1), encoder_hidden)\n",
        "        return c"
      ],
      "metadata": {
        "id": "BaiMyENGmxoS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}