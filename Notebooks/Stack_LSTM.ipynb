{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>IST 597 Foundations of Deep Learning</center></h1>\n",
        "\n",
        "---\n",
        "\n",
        "<h2><center>Stack Augmented Recurrent Neural Networks</center><h2>\n",
        "<h3><center>Neisarg Dave</center><h3>"
      ],
      "metadata": {
        "id": "7m8SmKIDgulg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stack Augmented LSTM\n",
        "\n",
        "**LSTM Equations**\n",
        "\n",
        "$$\n",
        "  i_t = \\sigma(U_ix_t + W_ih_{t-1} + b_i) \\\\\n",
        "  f_t = \\sigma(U_fx_t + W_fh_{t-1} + b_f) \\\\\n",
        "  o_t = \\sigma(U_ox_t + W_oh_{t-1} + b_o)\n",
        "$$\n",
        "\n",
        "$$\n",
        "  \\tilde{c}_t = \\tanh(U_cx_t + W_ch_{t-1} + b_c)\n",
        "$$\n",
        "\n",
        "$$\n",
        "  c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
        "$$\n",
        "\n",
        "$$\n",
        "  h_t = o_t \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "**Stack Augmented LSTM**\n",
        "\n",
        "Let stack $S_t$ represent the state of augmented stack at time $t$. \\\\\n",
        "Stack $S_t$ is a 2D Tensor of dimensions: *stack_depth x stack_dim* for each sample \\\\\n",
        "The each time step $t$ we use the top most vector in $S_{t-1}$, i.e. $S^{(0)}_{t-1}$ to compute gates and LSTM cell memory. \\\\\n",
        "\n",
        "The LSTM Equations get modified to the following equations: \\\\\n",
        " \\\\\n",
        "$$\n",
        "  i_t = \\sigma(U_ix_t + W_ih_{t-1} + V_iS^{(0)}_{t-1} + b_i) \\\\\n",
        "  f_t = \\sigma(U_fx_t + W_fh_{t-1} + V_fS^{(0)}_{t-1} + b_f) \\\\\n",
        "  o_t = \\sigma(U_ox_t + W_oh_{t-1} + V_oS^{(0)}_{t-1} + b_o)\n",
        "$$\n",
        "\n",
        "$$\n",
        "  \\tilde{c}_t = \\tanh(U_cx_t + W_ch_{t-1} + V_cS^{(0)}_{t-1} + b_c)\n",
        "$$\n",
        "\n",
        "$$\n",
        "  c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
        "$$\n",
        "\n",
        "$$\n",
        "  h_t = o_t \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "**Stack Update Equations**\n",
        "+ First we compute action vector. Its a vector of size 3 representing probabilities for the three stack options:\n",
        "  + PUSH\n",
        "  + POP\n",
        "  + NO-OP\n",
        "\n",
        "$$\n",
        "a_t = softmax(W_ah_t + b_a)\n",
        "$$\n",
        "\n",
        "+ Now we compute the stack vector for PUSH operation\n",
        "\n",
        "$$\n",
        "p = sigmoid(W_sh_t + b_s)\n",
        "$$\n",
        "\n",
        "+ update stack\n",
        "$$\n",
        "S_t = a[0]*[p ; S_{t-1}[:-1]] + a[1]*[S_{t-1}[1:]; 0] + a[2]*S_{t-1}\n",
        "$$"
      ],
      "metadata": {
        "id": "JOH0iuA3iAYh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WOqln6OgtwY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init, Parameter\n",
        "import math\n",
        "\n",
        "\n",
        "class StackLSTMlayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=False, dropout=0, eval_dropout = False):\n",
        "        super(StackLSTMlayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.WX = Parameter(torch.empty(self.in_features, 4*self.out_features))\n",
        "        self.WH = Parameter(torch.empty(self.out_features, 4*self.out_features))\n",
        "        self.WS = Parameter(torch.empty(self.out_features, 4*self.out_features))\n",
        "\n",
        "        self.action = nn.Sequential(nn.Linear(self.out_features, 3), nn.Softmax(dim = -1))\n",
        "        self.H2S = nn.Sequential(nn.Linear(self.out_features, self.out_features), nn.Sigmoid())\n",
        "\n",
        "        self.bias = Parameter(torch.empty(4*self.out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layers in self.action:\n",
        "            layers.reset_parameters()\n",
        "\n",
        "        for layers in self.H2S:\n",
        "            layers.reset_parameters()\n",
        "\n",
        "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.WX.t())\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "        init.normal_(self.WX, mean=0, std=0.1)\n",
        "        init.normal_(self.WH, mean=0, std=0.1)\n",
        "        init.normal_(self.WS, mean=0, std=0.1)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden, stack, mask):\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            hidden = self.inner(input[i], hidden, stack[:, 0])\n",
        "            stack = self.update_stack(hidden, stack, mask)\n",
        "\n",
        "            output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n",
        "\n",
        "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
        "\n",
        "        return hidden, output, stack\n",
        "\n",
        "    def update_stack(self, hidden, stack, mask):\n",
        "        action = self.action(hidden[0])\n",
        "        if mask is not None:\n",
        "            # we want no op where mask is 0\n",
        "            mask_no_op = torch.hstack([torch.zeros([mask.shape[0], 2], device=stack.device), (1-mask.type(torch.int)).unsqueeze(dim = -1)])\n",
        "            mask = mask.unsqueeze(dim = -1).expand(*action.size())\n",
        "            action = mask*action + mask_no_op\n",
        "\n",
        "        action = action.unsqueeze(dim = -1).unsqueeze(dim = -1).expand(stack.shape[0], 3, stack.shape[1], stack.shape[2])\n",
        "\n",
        "        new_inp = self.H2S(hidden[0]).unsqueeze(dim = 1)\n",
        "\n",
        "        pushed_stack = torch.cat([new_inp, stack[:, :-1]], dim = 1)\n",
        "        popped_stack = torch.cat([stack[:, 1:], torch.zeros([stack.shape[0], 1, self.out_features], device = stack.device)], dim = 1)\n",
        "        noop_stack = stack\n",
        "\n",
        "        stack = action[:, 0]*pushed_stack + action[:, 1]*popped_stack + action[:, 2]*noop_stack\n",
        "\n",
        "        return stack\n",
        "\n",
        "    def inner(self, input, hidden, stack):\n",
        "        h, c = hidden\n",
        "        wx = torch.matmul(input, self.WX)\n",
        "        wh = torch.matmul(h, self.WH)\n",
        "        ws = torch.matmul(stack, self.WS)\n",
        "        all_sum = wx + wh + ws\n",
        "        if self.bias is not None:\n",
        "            all_sum += self.bias\n",
        "        i, f, g, o = torch.chunk(all_sum, 4, dim=1)\n",
        "        i, f, g, o = torch.sigmoid(i), torch.sigmoid(f), torch.tanh(g), torch.sigmoid(o)\n",
        "        c = f*c + i*g\n",
        "        h = o*torch.tanh(c)\n",
        "        hidden = (h, c)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO:\n",
        "+ Implement other memory structures with  RNN\n",
        "  + Linked List\n",
        "  + 2 Stacks\n",
        "  + Tape"
      ],
      "metadata": {
        "id": "1Nl21gWXqycB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Cell behaves like a bounded stack\n",
        "https://nlp.stanford.edu/~johnhew/rnns-hierarchy.html"
      ],
      "metadata": {
        "id": "7bkE9S5StpO5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9MskmJ4PrIEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}